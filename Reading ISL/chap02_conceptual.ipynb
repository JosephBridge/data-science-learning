{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Statistical Learning\n",
    "\n",
    "## Conceptual\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Performance of a flexible statistical learning method (F) vs an inflexible method (I)\n",
    "\n",
    "- (a) The sample size $n$ is extremely large, the number of predictors $p$ is small: F better than I.\n",
    "- (b) $n$ small, $p$ large: F worse than I (there may be overfitting).\n",
    "- (c) Relationship predictors and response is highly non-linear: F better than I.\n",
    "- (d) $\\sigma^2 = \\text{Var}(\\epsilon)$ is extremely high: F worse than I (there may be overfitting).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Classification or regression? Inference or prediction? Provide $n$ and $p$.\n",
    "- (a) Regression. Inference. $n=500, p=3$.\n",
    "- (b) Classification. Prediction. $n=20, p=13$.\n",
    "- (c) Regression. Prediction. $n=52$ (number of weeks in 2012), $p=4$.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** Skech of relation between: bias, variance, training error, test error and Bayes (irreducible) error curves\n",
    "\n",
    "$$\n",
    "E\\left(y_0 - \\hat{f}(x_0)\\right)^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)\n",
    "$$\n",
    "\n",
    "![Relation between bias-variance](./fig/chap2-ex3a.png)\n",
    "\n",
    "- **Training error**: decreases if the flexibility increases because we try to fit more accurently the training set.\n",
    "- **Test error**: firstly decreases upto some point and then increases because of the overfitting problem.\n",
    "- **Bias**: the more complex model, the less bias error we get.\n",
    "- **Variance**: opposite to bias.\n",
    "- **Bayes (irreducible) error**: steady. Note that, the point where the irreducible curve is closest to the test error curve is the optimal operating point for this system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Reak-life examples for SL\n",
    "    \n",
    "- (a) Classification\n",
    "\n",
    "    1. stock market price direction, prediction, response: up, down. input: yesterday's price movement change, two previous day price movement change, etc.\n",
    "    2. illness classification, inference, response: ill, healthy, input: resting heart rate, resting breath rate, mile run time\n",
    "    3. car part replacement, prediction, response: needs to be replace, good, input: age of part, mileage used for, current amperage\n",
    "\n",
    "- (b) Regression \n",
    "\n",
    "    1. CEO salary. inference. predictors: age, industry experience, industry,\n",
    "    years of education. response: salary.\n",
    "\n",
    "    2. car part replacement. inference. response: life of car part. predictors: age\n",
    "    of part, mileage used for, current amperage.\n",
    "\n",
    "    3. illness classification, prediction, response: age of death,\n",
    "    input: current age, gender, resting heart rate, resting breath rate, mile run\n",
    "    time.\n",
    "\n",
    "- (c) Cluster \n",
    "\n",
    "    1. cancer type clustering. diagnose cancer types more accurately.\n",
    "\n",
    "    2. Netflix movie recommendations. recommend movies based on users who have\n",
    "    watched and rated similar movies.\n",
    "\n",
    "    3. marketing survey. clustering of demographics for a product(s) to see which\n",
    "    clusters of consumers buy which products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** Very flexible vs less flexible\n",
    "\n",
    "- The advantages for a very flexible approach for regression or classification are obtaining a better fit for non-linear models, decreasing bias.\n",
    "- The disadvantages for a very flexible approach for regression or classification are requires estimating a greater number of parameters, follow the noise too closely (overfit), increasing variance.\n",
    "- A more flexible approach would be preferred to a less flexible approach when we are interested in prediction and not the interpretability of the results.\n",
    "- A less flexible approach would be preferred to a more flexible approach when we are interested in inference and the interpretability of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** Parametric vs non-parametric\n",
    "\n",
    "|              | Parametric methods                                                                        | Non-parametric methods                                                                                       |\n",
    "|--------------|-------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|\n",
    "| Definition   | Give an explicit approximation function $\\hat{f}$ of $f$.                                 | Seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly. |\n",
    "| Advantage    | Simplifies the problem of estimating $f$ because we just need to estimate the parameters. | Avoid the assumption of a form of $f$ and have potential change to fit to any shapes of $f$.                 |\n",
    "| Disadvantage | The model we choose will usually not match the true form of $f$.                          | Need a very large number of observations.                                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** K-nearest neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  x3  y\n",
       "0   0   3   0  R\n",
       "1   2   0   0  R\n",
       "2   0   1   3  R\n",
       "3   0   1   2  G\n",
       "4  -1   0   1  G\n",
       "5   1   1   1  R"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_dict = {'x1': [0,2,0,0,-1,1],\n",
    "           'x2': [3,0,1,1,0,1],\n",
    "           'x3': [0,0,3,2,1,1],\n",
    "           'y': ['R','R','R','G','G','R']\n",
    "          }\n",
    "df = pd.DataFrame(df_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (a) Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.        ,  2.        ,  3.16227766,  2.23606798,  1.41421356,\n",
       "        1.73205081])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = (df.values[:,:3] - np.array([0,0,0])) ** 2\n",
    "dist = dist.sum(axis=1).astype('float')\n",
    "dist = np.sqrt(dist)\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (b) $K=1$: From above distance result, the shortest distance is $1.4$ which is corresponding to the point number 5. The prediction is Green.\n",
    "- (c) $K=3$: three corresponding closet points are 2,5,6 (Red, Green, Red). The prediction is Red.\n",
    "- (d) $K$ needs to be small for highly non-linear Bayes decision boundary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
